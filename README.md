The application opens with a live preview powered by the Pi camera, where users can interact through gestures. In this default state, the gesture loop is active: frames are captured, downscaled for efficiency, and processed every fifth frame to detect input. A single‑hand gesture triggers the capture routine, sending a request to the server to store an image with the desired exposure. To prevent accidental repeats, a cooldown timer ensures that captures are spaced apart.

Navigation is seamless. When the user moves into review or gallery mode, the gesture loop is paused, allowing them to browse or evaluate images without interference from background detection. Returning to the preview automatically re‑enables gesture input, maintaining a smooth, appliance‑like feel. This flag‑driven control ensures that the system responds only when appropriate, avoiding frozen previews or stray triggers.

For quitting, a dedicated gesture — such as detecting two hands raised — signals the system to cleanly shut down. Resources like the camera and Mediapipe graph are released before exit, ensuring stability and preventing stack traces. This gesture‑driven exit complements the capture and navigation flow, giving users a complete hands‑free experience.

Overall, the design emphasizes robustness and accessibility: efficient frame handling, clear state transitions, and intuitive gestures combine to create a responsive tool that feels natural to use. The experience is fluid, with each mode — preview, review, gallery, and quit — integrated into a coherent loop that balances performance with usability.